{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfee1ab",
   "metadata": {},
   "source": [
    "# Proyecto – Detección de Comunidades\n",
    "\n",
    "Esta guía proporciona una estructura para el estudio del análisis de comunidades en redes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270a736",
   "metadata": {},
   "source": [
    "* Asignatura: Base de datos 3\n",
    "* Profesor: Ana Aguilera Faraco\n",
    "* Ayudante: Fernanda Fuentes\n",
    "* Fecha: 7 de noviembre del 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9b2be",
   "metadata": {},
   "source": [
    "## Estructura del Estudio\n",
    "\n",
    "1. Descripción y Preparación del Dataset\n",
    "2. Análisis de Estructura de Red\n",
    "3. Métodos de Detección de Comunidades\n",
    "4. Visualización de Comunidades\n",
    "5. Métricas y Evaluación de Comunidades\n",
    "6. Análisis Temporal de Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30a57",
   "metadata": {},
   "source": [
    "# 1. Descripción y Preparación del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2662449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dbc0aa1",
   "metadata": {},
   "source": [
    "## 1.1 Tipos de Redes para Análisis\n",
    "* Redes sociales\n",
    "* Redes biológicas\n",
    "* Redes de información\n",
    "* Redes tecnológicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47adff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de utilidad para manejo de tipos de redes\n",
    "import networkx as nx\n",
    "\n",
    "def identify_network_type(G):\n",
    "    \"\"\"Identifica características básicas de la red\"\"\"\n",
    "    info = {\n",
    "        \"num_nodes\": G.number_of_nodes(),\n",
    "        \"num_edges\": G.number_of_edges(),\n",
    "        \"is_directed\": G.is_directed(),\n",
    "        \"is_weighted\": any(isinstance(w, (int, float)) \n",
    "                          for _, _, w in G.edges(data=\"weight\", default=1)),\n",
    "        \"has_self_loops\": any(G.has_edge(n, n) for n in G.nodes()),\n",
    "        \"density\": nx.density(G),\n",
    "        \"avg_degree\": sum(dict(G.degree()).values()) / G.number_of_nodes()\n",
    "    }\n",
    "    return info\n",
    "\n",
    "def print_network_info(info):\n",
    "    \"\"\"Imprime información de la red en formato legible\"\"\"\n",
    "    print(\"Características de la Red:\")\n",
    "    print(f\"- Nodos: {info['num_nodes']}\")\n",
    "    print(f\"- Enlaces: {info['num_edges']}\")\n",
    "    print(f\"- Tipo: {'Dirigida' if info['is_directed'] else 'No dirigida'}\")\n",
    "    print(f\"- Enlaces con peso: {'Sí' if info['is_weighted'] else 'No'}\")\n",
    "    print(f\"- Auto-enlaces: {'Sí' if info['has_self_loops'] else 'No'}\")\n",
    "    print(f\"- Densidad: {info['density']:.4f}\")\n",
    "    print(f\"- Grado promedio: {info['avg_degree']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefcb9d",
   "metadata": {},
   "source": [
    "## 1.2 Requisitos de los Datos\n",
    "* Formato de datos de entrada\n",
    "  - Archivos de texto plano (.txt, .csv)\n",
    "  - Listas de enlaces (edge lists)\n",
    "  - Matrices de adyacencia\n",
    "* Estructuras de grafos\n",
    "  - Dirigidos vs No dirigidos\n",
    "  - Con peso vs Sin peso\n",
    "  - Atributos de nodos y enlaces\n",
    "* Consideraciones de formato\n",
    "  - Separadores (espacios, comas, tabs)\n",
    "  - Encabezados y metadatos\n",
    "  - Codificación de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab89faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para validación y limpieza de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_edge_list(file_path, sep=None, header=None):\n",
    "    \"\"\"Valida formato y contenido de una lista de enlaces\"\"\"\n",
    "    try:\n",
    "        # Intentar leer el archivo\n",
    "        df = pd.read_csv(file_path, sep=sep, header=header, error_bad_lines=False)\n",
    "        \n",
    "        # Verificar formato básico\n",
    "        if df.shape[1] < 2:\n",
    "            return False, \"El archivo debe tener al menos 2 columnas (origen, destino)\"\n",
    "        \n",
    "        # Verificar tipos de datos\n",
    "        if not (df.iloc[:, 0].dtype in ['int64', 'object'] and \n",
    "                df.iloc[:, 1].dtype in ['int64', 'object']):\n",
    "            return False, \"Los identificadores deben ser números o texto\"\n",
    "        \n",
    "        # Verificar duplicados\n",
    "        duplicates = df.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            print(f\"Advertencia: {duplicates} enlaces duplicados encontrados\")\n",
    "        \n",
    "        return True, f\"Archivo válido: {df.shape[0]} enlaces, {df.nunique().iloc[0]} nodos origen, {df.nunique().iloc[1]} nodos destino\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, f\"Error al leer el archivo: {str(e)}\"\n",
    "\n",
    "def clean_edge_list(file_path, sep=None, header=None):\n",
    "    \"\"\"Limpia y normaliza una lista de enlaces\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=sep, header=header)\n",
    "        \n",
    "        # Eliminar duplicados\n",
    "        df_clean = df.drop_duplicates()\n",
    "        \n",
    "        # Eliminar auto-enlaces si se desea\n",
    "        df_clean = df_clean[df_clean.iloc[:, 0] != df_clean.iloc[:, 1]]\n",
    "        \n",
    "        # Convertir a identificadores numéricos si son strings\n",
    "        if df_clean.iloc[:, 0].dtype == 'object':\n",
    "            nodes = pd.concat([df_clean.iloc[:, 0], df_clean.iloc[:, 1]]).unique()\n",
    "            node_map = {node: i for i, node in enumerate(nodes)}\n",
    "            df_clean.iloc[:, 0] = df_clean.iloc[:, 0].map(node_map)\n",
    "            df_clean.iloc[:, 1] = df_clean.iloc[:, 1].map(node_map)\n",
    "        \n",
    "        return df_clean, \"Limpieza completada exitosamente\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None, f\"Error en la limpieza: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f3467",
   "metadata": {},
   "source": [
    "## 1.3 Preparación de Datos\n",
    "* Limpieza de datos\n",
    "  - Eliminación de duplicados\n",
    "  - Manejo de valores faltantes\n",
    "  - Corrección de formatos\n",
    "* Normalización\n",
    "  - Estandarización de identificadores\n",
    "  - Conversión de tipos de datos\n",
    "  - Unificación de formatos\n",
    "* Validación\n",
    "  - Verificación de integridad\n",
    "  - Control de consistencia\n",
    "  - Detección de anomalías\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Lectura y validación de datos\n",
    "* Limpieza y normalización\n",
    "* Creación de estructura de grafo\n",
    "* Verificación de integridad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e78674",
   "metadata": {},
   "source": [
    "## 1.4 Estrategias de Muestreo\n",
    "* Muestreo de nodos\n",
    "  - Selección aleatoria\n",
    "  - Muestreo estratificado\n",
    "  - Basado en grado\n",
    "* Muestreo de enlaces\n",
    "  - Selección aleatoria de enlaces\n",
    "  - Preservación de comunidades\n",
    "  - Muestreo por inducción\n",
    "* Control de calidad\n",
    "  - Tamaño de muestra\n",
    "  - Preservación de propiedades\n",
    "  - Validación estadística\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Técnicas de muestreo\n",
    "* Validación de la muestra\n",
    "* Análisis comparativo\n",
    "* Almacenamiento eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones avanzadas de muestreo\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "def random_node_sample(G, sample_size, seed=None):\n",
    "    \"\"\"Muestreo aleatorio de nodos\"\"\"\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    nodes = list(G.nodes())\n",
    "    sampled_nodes = random.sample(nodes, min(sample_size, len(nodes)))\n",
    "    return G.subgraph(sampled_nodes).copy()\n",
    "\n",
    "def degree_based_sample(G, sample_size, method='top', seed=None):\n",
    "    \"\"\"Muestreo basado en grado (top/bottom/random weighted)\"\"\"\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    degrees = dict(G.degree())\n",
    "    if method == 'top':\n",
    "        # Tomar los nodos de mayor grado\n",
    "        nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "        sampled_nodes = [n for n, _ in nodes[:sample_size]]\n",
    "    elif method == 'bottom':\n",
    "        # Tomar los nodos de menor grado\n",
    "        nodes = sorted(degrees.items(), key=lambda x: x[1])\n",
    "        sampled_nodes = [n for n, _ in nodes[:sample_size]]\n",
    "    else:\n",
    "        # Muestreo ponderado por grado\n",
    "        total_degree = sum(degrees.values())\n",
    "        weights = [d/total_degree for d in degrees.values()]\n",
    "        sampled_nodes = random.choices(list(degrees.keys()), \n",
    "                                     weights=weights, \n",
    "                                     k=min(sample_size, len(G)))\n",
    "    \n",
    "    return G.subgraph(sampled_nodes).copy()\n",
    "\n",
    "def edge_based_sample(G, sample_size, method='random', seed=None):\n",
    "    \"\"\"Muestreo basado en enlaces\"\"\"\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    edges = list(G.edges())\n",
    "    if method == 'random':\n",
    "        # Muestreo aleatorio de enlaces\n",
    "        sampled_edges = random.sample(edges, min(sample_size, len(edges)))\n",
    "    else:\n",
    "        # Muestreo por inducción (preserva estructura local)\n",
    "        sampled_edges = []\n",
    "        current_edges = set(random.sample(edges, 1))\n",
    "        while len(sampled_edges) < sample_size and current_edges:\n",
    "            edge = current_edges.pop()\n",
    "            sampled_edges.append(edge)\n",
    "            # Agregar enlaces adyacentes\n",
    "            u, v = edge\n",
    "            current_edges.update(set(G.edges(u)) | set(G.edges(v)) - set(sampled_edges))\n",
    "    \n",
    "    # Construir subgrafo con los enlaces muestreados\n",
    "    H = nx.Graph()\n",
    "    H.add_edges_from(sampled_edges)\n",
    "    return H\n",
    "\n",
    "def validate_sample(G_original, G_sample, metrics=None):\n",
    "    \"\"\"Valida la calidad de la muestra comparando métricas\"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = ['density', 'clustering', 'degree_distribution']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if 'density' in metrics:\n",
    "        results['density'] = {\n",
    "            'original': nx.density(G_original),\n",
    "            'sample': nx.density(G_sample)\n",
    "        }\n",
    "    \n",
    "    if 'clustering' in metrics:\n",
    "        results['clustering'] = {\n",
    "            'original': nx.average_clustering(G_original),\n",
    "            'sample': nx.average_clustering(G_sample)\n",
    "        }\n",
    "    \n",
    "    if 'degree_distribution' in metrics:\n",
    "        orig_degrees = [d for _, d in G_original.degree()]\n",
    "        sample_degrees = [d for _, d in G_sample.degree()]\n",
    "        results['avg_degree'] = {\n",
    "            'original': sum(orig_degrees) / len(orig_degrees),\n",
    "            'sample': sum(sample_degrees) / len(sample_degrees)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f58abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga y creación de muestra para edgelist grande\n",
    "import os\n",
    "import math\n",
    "import networkx as nx\n",
    "\n",
    "DATAFILE = 'wiki-topcats.txt'  # ajustar ruta si procede\n",
    "SAMPLE_PATH = 'wiki-topcats-sample.txt'\n",
    "\n",
    "def file_size_mb(path):\n",
    "    try:\n",
    "        return os.path.getsize(path) / (1024*1024)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_sample(path, max_lines=200000, save_sample=False):\n",
    "    G = nx.Graph()\n",
    "    if not os.path.exists(path):\n",
    "        print('Archivo no encontrado:', path)\n",
    "        return G\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines and i >= max_lines:\n",
    "                break\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            u, v = parts[0], parts[1]\n",
    "            G.add_edge(u, v)\n",
    "    if save_sample:\n",
    "        with open(SAMPLE_PATH, 'w', encoding='utf-8') as out:\n",
    "            for u, v in G.edges():\n",
    "                out.write(f\"{u} {v}\\n\")\n",
    "        print('Muestra guardada en', SAMPLE_PATH)\n",
    "    return G\n",
    "\n",
    "# Selección automática: si el archivo es muy grande, usar muestra\n",
    "size = file_size_mb(DATAFILE)\n",
    "if size is None:\n",
    "    print('No se encontró', DATAFILE, 'en el directorio actual. Ajusta DATAFILE si es necesario.')\n",
    "    G = nx.Graph()\n",
    "else:\n",
    "    print(f'File size: {size:.1f} MB')\n",
    "    if size > 150:\n",
    "        print('Archivo grande detectado. Construyendo muestra de 200k líneas...')\n",
    "        G = build_sample(DATAFILE, max_lines=200000, save_sample=False)\n",
    "    else:\n",
    "        print('Archivo moderado/pequeño. Construyendo grafo completo...')\n",
    "        G = build_sample(DATAFILE, max_lines=None, save_sample=False)\n",
    "\n",
    "print('Grafo construido: Nodos=', G.number_of_nodes(), 'Aristas=', G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8292e7",
   "metadata": {},
   "source": [
    "# 2. Análisis de Estructura de Red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251ad70",
   "metadata": {},
   "source": [
    "## 2.1 Métricas Básicas\n",
    "* Densidad de la red\n",
    "* Distribución de grado\n",
    "* Coeficiente de clustering\n",
    "* Longitud de camino promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c249f",
   "metadata": {},
   "source": [
    "## 2.2 Medidas de Centralidad\n",
    "* Centralidad de grado\n",
    "* Centralidad de intermediación\n",
    "* Centralidad de cercanía\n",
    "* Centralidad de eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842420ff",
   "metadata": {},
   "source": [
    "## 2.3 Propiedades Estructurales\n",
    "* Componentes conectados\n",
    "* Puentes y puntos de articulación\n",
    "* Triángulos y coeficientes locales\n",
    "* Patrones de conectividad\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Cálculo de métricas básicas\n",
    "* Análisis de centralidad\n",
    "* Visualización de distribuciones\n",
    "* Identificación de estructuras clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas básicas y centralidad (muestra)\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "if 'G' not in globals() or G is None or G.number_of_nodes() == 0:\n",
    "    print('Ejecuta la celda de carga de datos primero.')\n",
    "else:\n",
    "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
    "    print(f'Nodos={n} Aristas={m}')\n",
    "\n",
    "    # Grado\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    print('Grado: min', min(degrees), 'max', max(degrees), 'mean', round(statistics.mean(degrees), 2))\n",
    "\n",
    "    # Densidad\n",
    "    print('Densidad:', nx.density(G))\n",
    "\n",
    "    # Clustering\n",
    "    try:\n",
    "        clustering_vals = nx.clustering(G)\n",
    "        print('Clustering medio:', round(statistics.mean(clustering_vals.values()), 4))\n",
    "    except Exception:\n",
    "        print('Error calculando clustering (grafo dirigido o muy grande)')\n",
    "\n",
    "    # Componentes\n",
    "    comps = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    print('Número de componentes:', len(comps))\n",
    "    print('Tamaño de la componente gigante:', len(comps[0]) if comps else 0)\n",
    "\n",
    "    # Triángulos (local)\n",
    "    try:\n",
    "        tri = nx.triangles(G)\n",
    "        print('Número total de triángulos (sum/3):', sum(tri.values()) // 3)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Centralidades (rápido: grado; aproximado: betweenness con k)\n",
    "    deg_cent = nx.degree_centrality(G)\n",
    "    top_deg = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print('\\nTop 10 nodos por centralidad de grado:')\n",
    "    for node, val in top_deg:\n",
    "        print(node, round(val, 4))\n",
    "\n",
    "    try:\n",
    "        # k se ajusta para aproximar si el grafo es grande\n",
    "        k = 200 if n > 5000 else None\n",
    "        bet = nx.betweenness_centrality(G, k=k)\n",
    "        top_bet = sorted(bet.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print('\\nTop 10 nodos por betweenness (aprox):')\n",
    "        for node, val in top_bet:\n",
    "            print(node, round(val, 4))\n",
    "    except Exception as e:\n",
    "        print('Betweenness skipped or failed:', e)\n",
    "\n",
    "    # Histograma de grado si matplotlib disponible\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(degrees, bins=50, color='#3c8dbc')\n",
    "        plt.xlabel('Grado')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribución de grado (muestra)')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('No se pudo graficar distribución de grado:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4620d97",
   "metadata": {},
   "source": [
    "# 3. Métodos de Detección de Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214a660",
   "metadata": {},
   "source": [
    "## 3.1 Algoritmos Fundamentales\n",
    "* Método de Louvain\n",
    "* Algoritmo de Girvan-Newman\n",
    "* Propagación de Etiquetas\n",
    "* Optimización de Modularidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16360b31",
   "metadata": {},
   "source": [
    "## 3.2 Características de Algoritmos\n",
    "* Complejidad computacional\n",
    "* Escalabilidad\n",
    "* Resolución límite\n",
    "* Ventajas y limitaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0db01a",
   "metadata": {},
   "source": [
    "## 3.3 Selección de Método\n",
    "* Criterios de selección\n",
    "* Tamaño de la red\n",
    "* Tipo de comunidades esperadas\n",
    "* Recursos computacionales\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Algoritmos de detección\n",
    "* Comparación de resultados\n",
    "* Análisis de rendimiento\n",
    "* Visualización de comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d128f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de comunidades: Louvain, Label Propagation y demo Girvan-Newman\n",
    "from collections import Counter\n",
    "\n",
    "if 'G' not in globals() or G is None or G.number_of_nodes() == 0:\n",
    "    print('Ejecuta la celda de carga de datos primero.')\n",
    "else:\n",
    "    partition = None\n",
    "    # Louvain (recomendado)\n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        counts = Counter(partition.values())\n",
    "        print('Comunidades detectadas (Louvain):', len(counts))\n",
    "        print('Top 5 comunidades (Louvain):', counts.most_common(5))\n",
    "    except Exception as e:\n",
    "        print('python-louvain no disponible o falló:', e)\n",
    "        partition = None\n",
    "\n",
    "    # Label Propagation (rápido, sin dependencias externas)\n",
    "    try:\n",
    "        from networkx.algorithms.community import label_propagation\n",
    "        lp_comms = list(label_propagation.label_propagation_communities(G))\n",
    "        print('Comunidades (Label Propagation) detectadas:', len(lp_comms))\n",
    "    except Exception as e:\n",
    "        print('Label Propagation falló:', e)\n",
    "\n",
    "    # Girvan-Newman demo en subgrafo pequeño (solo ilustrativo)\n",
    "    try:\n",
    "        from networkx.algorithms import community as nx_comm\n",
    "        nodes_small = list(sorted(G.nodes(), key=lambda n: G.degree(n), reverse=True))[:300]\n",
    "        G_small = G.subgraph(nodes_small).copy()\n",
    "        comp_gen = nx_comm.girvan_newman(G_small)\n",
    "        top = tuple(sorted(c) for c in next(comp_gen))\n",
    "        print('Demo Girvan-Newman (subgrafo 300 nodos): comunidades en primer corte =', len(top))\n",
    "    except Exception as e:\n",
    "        print('Girvan-Newman demo falló:', e)\n",
    "\n",
    "    # leave partition in globals for later cells\n",
    "    if partition:\n",
    "        print('Partition dictionary available as `partition` (node -> community)')\n",
    "    else:\n",
    "        print('No partition available from Louvain; consider installing python-louvain if deseas esa partición.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b6b30",
   "metadata": {},
   "source": [
    "# 4. Visualización de Comunidades\n",
    "\n",
    "Técnicas efectivas para visualizar y comprender las estructuras comunitarias:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bc82b",
   "metadata": {},
   "source": [
    "## 4.1 Técnicas de Visualización\n",
    "* Layouts de fuerza dirigida\n",
    "* Visualización jerárquica\n",
    "* Mapas de calor\n",
    "* Representaciones circulares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc2b63",
   "metadata": {},
   "source": [
    "## 4.2 Estrategias de Diseño\n",
    "* Codificación por colores\n",
    "* Agrupamiento visual\n",
    "* Escalado de nodos\n",
    "* Filtrado interactivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de81e3",
   "metadata": {},
   "source": [
    "## 4.3 Interpretación Visual\n",
    "* Patrones estructurales\n",
    "* Roles de nodos\n",
    "* Conexiones entre comunidades\n",
    "* Anomalías y casos especiales\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Creación de visualizaciones\n",
    "* Personalización de layouts\n",
    "* Interactividad\n",
    "* Exportación de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_29360\\3677995847.py:22: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = plt.cm.get_cmap('tab20', max(1, len(unique_comms)))\n"
     ]
    }
   ],
   "source": [
    "# Visualización por comunidad (usa `partition` si está disponible)\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "if 'G' not in globals() or G is None or G.number_of_nodes() == 0:\n",
    "    print('Ejecuta la celda de carga de datos primero.')\n",
    "else:\n",
    "    # Determinar mapa de comunidades\n",
    "    if 'partition' in globals() and partition:\n",
    "        comm_map = partition\n",
    "    else:\n",
    "        # fallback: cada componente es su propia 'comunidad'\n",
    "        comm_map = {n: i for i, comp in enumerate(nx.connected_components(G)) for n in comp}\n",
    "\n",
    "    # Seleccionar la componente gigante para visualizar\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    Gv = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    # Recalcular comm_map limitado a Gv\n",
    "    nodes = list(Gv.nodes())\n",
    "    unique_comms = sorted({comm_map.get(n, 0) for n in nodes})\n",
    "    cmap = plt.cm.get_cmap('tab20', max(1, len(unique_comms)))\n",
    "    color_list = [cmap(unique_comms.index(comm_map.get(n, 0))) for n in nodes]\n",
    "\n",
    "    pos = nx.spring_layout(Gv, seed=42, k=0.1, iterations=50)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw_networkx_nodes(Gv, pos, node_size=20, node_color=color_list)\n",
    "    nx.draw_networkx_edges(Gv, pos, alpha=0.2, width=0.4)\n",
    "    plt.title('Componente gigante coloreada por comunidad')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar tamaños de comunidades en la componente gigante\n",
    "    size_by_comm = Counter(comm_map.get(n, 0) for n in nodes)\n",
    "    print('Comunidades en la componente gigante (top 10):', size_by_comm.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e10f41",
   "metadata": {},
   "source": [
    "# 5. Métricas y Evaluación de Comunidades\n",
    "\n",
    "Métodos para evaluar la calidad de las comunidades detectadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51f63",
   "metadata": {},
   "source": [
    "## 5.1 Métricas de Calidad\n",
    "* Modularidad\n",
    "* Conductancia\n",
    "* Cobertura\n",
    "* Rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c10e88",
   "metadata": {},
   "source": [
    "## 5.2 Evaluación Interna\n",
    "* Densidad intra-comunidad\n",
    "* Dispersión inter-comunidad\n",
    "* Cohesión\n",
    "* Separación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3352e",
   "metadata": {},
   "source": [
    "## 5.3 Evaluación Externa\n",
    "* Ground truth comparación\n",
    "* Índice de Rand ajustado\n",
    "* Información mutua normalizada\n",
    "* F-measure\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Cálculo de métricas\n",
    "* Comparación de resultados\n",
    "* Validación de comunidades\n",
    "* Visualización de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de comunidades: modularidad, conductancia y métricas externas si hay ground truth\n",
    "from collections import defaultdict\n",
    "\n",
    "def conductance(G, community_nodes):\n",
    "    cut = 0\n",
    "    volS = 0\n",
    "    for u in community_nodes:\n",
    "        volS += G.degree(u)\n",
    "        for v in G.neighbors(u):\n",
    "            if v not in community_nodes:\n",
    "                cut += 1\n",
    "    vol_rest = sum(dict(G.degree()).values()) - volS\n",
    "    denom = min(volS, vol_rest) if min(volS, vol_rest) > 0 else 1\n",
    "    return cut / denom\n",
    "\n",
    "if 'partition' in globals() and partition:\n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        mod = community_louvain.modularity(partition, G)\n",
    "        print('Modularidad (Louvain):', round(mod, 4))\n",
    "    except Exception as e:\n",
    "        print('No se pudo calcular modularidad (python-louvain):', e)\n",
    "\n",
    "    # Conductance for top communities\n",
    "    comms = defaultdict(list)\n",
    "    for n, c in partition.items():\n",
    "        comms[c].append(n)\n",
    "    top = sorted(comms.items(), key=lambda x: len(x[1]), reverse=True)[:3]\n",
    "    for cid, nodes in top:\n",
    "        print(f'Conductancia comunidad {cid} (tam={len(nodes)}):', round(conductance(G, set(nodes)), 4))\n",
    "\n",
    "else:\n",
    "    print('No hay partición disponible para evaluación. Ejecuta la celda de detección de comunidades.')\n",
    "\n",
    "# Evaluación externa si hay ground truth en dict `true_labels` (node -> label)\n",
    "try:\n",
    "    from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "    if 'true_labels' in globals() and 'partition' in globals() and partition:\n",
    "        nodes_common = [n for n in G.nodes() if n in true_labels and n in partition]\n",
    "        y_true = [true_labels[n] for n in nodes_common]\n",
    "        y_pred = [partition[n] for n in nodes_common]\n",
    "        print('NMI:', round(normalized_mutual_info_score(y_true, y_pred), 4))\n",
    "        print('ARI:', round(adjusted_rand_score(y_true, y_pred), 4))\n",
    "except Exception as e:\n",
    "    # sklearn puede no estar instalado o no haber ground truth\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6836a",
   "metadata": {},
   "source": [
    "## 6.1 Dinámica de Comunidades\n",
    "* Nacimiento y muerte\n",
    "* Fusión y división\n",
    "* Crecimiento y contracción\n",
    "* Estabilidad y cambio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0573056",
   "metadata": {},
   "source": [
    "## 6.2 Métricas Temporales\n",
    "* Supervivencia de comunidades\n",
    "* Tasa de cambio\n",
    "* Persistencia de membresía\n",
    "* Evolución estructural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d79994",
   "metadata": {},
   "source": [
    "## 6.3 Patrones de Evolución\n",
    "* Ciclos de vida\n",
    "* Puntos de transición\n",
    "* Factores de cambio\n",
    "* Predicción de evolución\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Análisis de series temporales\n",
    "* Tracking de comunidades\n",
    "* Visualización de evolución\n",
    "* Predicción de cambios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a8840",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusión\n",
    "Este notebook servirá como guía para el análisis de comunidades en redes, con implementaciones prácticas, ejemplos y visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9996153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple de slicing temporal (si hay timestamps en columna 3)\n",
    "from collections import defaultdict\n",
    "\n",
    "def slice_windows(path, window_size=86400, max_lines=None):\n",
    "    windows = defaultdict(list)\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines and i>=max_lines: break\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                if len(parts) >= 2:\n",
    "                    windows[0].append((parts[0], parts[1]))\n",
    "                continue\n",
    "            try:\n",
    "                t = int(parts[2])\n",
    "            except:\n",
    "                continue\n",
    "            win = (t//window_size)*window_size\n",
    "            windows[win].append((parts[0], parts[1]))\n",
    "    return windows\n",
    "\n",
    "print('Si tu edgelist incluye timestamps, usa slice_windows para crear snapshots por ventana.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
