{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dfee1ab",
   "metadata": {},
   "source": [
    "# Proyecto – Detección de Comunidades\n",
    "\n",
    "Esta guía proporciona una estructura para el estudio del análisis de comunidades en redes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270a736",
   "metadata": {},
   "source": [
    "* Asignatura: Base de datos 3\n",
    "* Profesor: Ana Aguilera Faraco\n",
    "* Ayudante: Fernanda Fuentes\n",
    "* Fecha: 7 de noviembre del 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498e5d9",
   "metadata": {},
   "source": [
    "**Integrantes**\n",
    "\n",
    "\n",
    "* Dante Chavez  dante.chavez@estudiantes.uv.cl\n",
    "* Nikolas Lagos  nikolas.lagos@estudiantes.uv.cl\n",
    "* Franko Moraga  franko.moraga@estudiantes.uv.cl\n",
    "* Miguel Espinoza  miguel.espinoza@estudiantes.uv.cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cc83f",
   "metadata": {},
   "source": [
    "**Presentacion**\n",
    "\n",
    "Este trabajo tiene como objetivo introducir y demostrar el concepto de detección de comunidades en redes complejas, un tema central en la minería de grafos y la ciencia de datos. A través de una explicación teórica y una implementación práctica en Python, se busca mostrar cómo identificar grupos de nodos altamente conectados dentro de una red, revelando patrones ocultos de relación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b8f40",
   "metadata": {},
   "source": [
    "**Objetivo General**\n",
    "\n",
    "Comprender y aplicar técnicas de detección de comunidades en redes, evaluando los resultados según la estructura del grafo.\n",
    "\n",
    "**Objetivos Específicos**\n",
    "\n",
    "* Introducir conceptos clave sobre redes y grafos en ciencia de datos.\n",
    "* Explicar la utilidad de detectar comunidades y sus aplicaciones.\n",
    "* Implementar algoritmos de detección en un dataset de red.\n",
    "* Visualizar y analizar los resultados mediante métricas y gráficos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fed60",
   "metadata": {},
   "source": [
    "**Conceptos Clave**\n",
    "\n",
    "* **Red:** Conjunto de elementos conectados entre sí. En ciencia de datos, se usa para modelar relaciones entre entidades (personas, productos, genes, etc.).\n",
    "* Grafo: Representación matemática de una red, formada por nodos (entidades) y aristas (relaciones).\n",
    "* Nodo: Elemento individual dentro de una red (por ejemplo, un usuario en una red social).\n",
    "* Arista: Conexión entre dos nodos, que puede ser dirigida o no, y tener peso.\n",
    "* **Detección de comunidades:** Conjunto de técnicas no supervisadas utilizadas para encontrar grupos de nodos altamente interconectados. Tambien es el proceso de identificar grupos de nodos con alta conectividad interna y ayuda a descubrir estructuras ocultas en redes complejas.\n",
    "\n",
    "* Modularidad: Métrica que evalúa la calidad de una partición en comunidades. Cuanto mayor sea, mejor definida está la comunidad.\n",
    "* Algoritmo Louvain: Método eficiente que agrupa nodos maximizando la modularidad.\n",
    "* Girvan–Newman: Algoritmo que elimina aristas con alta intermediación para dividir la red.\n",
    "* Label Propagation: Técnica rápida que asigna etiquetas a nodos y las propaga hasta estabilizarse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a8ae1",
   "metadata": {},
   "source": [
    "**Introducción**\n",
    "\n",
    "Este trabajo busca aplicar técnicas de **detección de comunidades en redes complejas**, utilizando un enfoque práctico a partir de un dataset real.  \n",
    "Para ello, se emplea un conjunto de datos del repositorio **[SNAP (Stanford Large Network Dataset Collection)](https://snap.stanford.edu/data/wiki-topcats.html)**, conocido por reunir redes utilizadas en investigación y análisis de grafos.\n",
    "\n",
    "El dataset elegido es **“Wikipedia network of top categories”**, que representa una red de hipervínculos entre artículos de Wikipedia recopilada en septiembre de 2011.  \n",
    "Cada nodo corresponde a una página, y cada enlace indica una relación entre ellas. Además, cada artículo pertenece a una o más categorías, las cuales se pueden considerar como **comunidades de referencia** (*ground-truth communities*), ya que reflejan agrupaciones reales dentro del sitio.\n",
    "\n",
    "La elección de este dataset se debe a principalmente:  \n",
    "- **Tamaño y complejidad:** con más de 1,7 millones de nodos y 28 millones de enlaces, permite analizar un sistema grande y realista.  \n",
    "- **Categorías como comunidades:** las categorías temáticas sirven como base para validar los resultados de los algoritmos de detección.  \n",
    "- **Interés y aplicabilidad:** el estudio de Wikipedia permite entender cómo se relacionan los temas y cómo se forman comunidades naturales dentro del conocimiento en línea.\n",
    "\n",
    "-Este cuaderno busca demostrar cómo los métodos de detección de comunidades pueden revelar la estructura interna de una red, utilizando un caso real y relevante como el de Wikipedia.\n",
    "\n",
    "-Yin, H., Benson, A. R., Leskovec, J., & Gleich, D. F. (2017). Local higher-order graph clustering. Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Recuperado de https://snap.stanford.edu/data/ [snap.stanford.edu]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1ce7a",
   "metadata": {},
   "source": [
    "**Estructura del Estudio**\n",
    "\n",
    "1. Descripción y Preparación del Dataset\n",
    "2. Análisis de Estructura de Red\n",
    "3. Métodos de Detección de Comunidades\n",
    "4. Visualización de Comunidades\n",
    "5. Métricas y Evaluación de Comunidades\n",
    "6. Análisis Temporal de Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea56547",
   "metadata": {},
   "source": [
    "**Librerias usadas:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c31c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # manejo y análisis de datos tabulares (DataFrame)\n",
    "import numpy as np  # cálculo numérico y arrays eficientes\n",
    "import networkx as nx  # construcción y análisis de grafos/redes\n",
    "import matplotlib.pyplot as plt  # visualización (plots, histogramas, visualización de grafos)\n",
    "import random  # muestreo aleatorio y funciones de aleatoriedad\n",
    "import os  # interacción con el sistema de archivos (comprobación, tamaño, rutas)\n",
    "import math  # funciones matemáticas básicas (log, sqrt, etc.)\n",
    "import statistics  # estadísticas básicas (mean, median, stdev)\n",
    "from collections import Counter, defaultdict  # Counter: conteos; defaultdict: diccionarios con valor por defecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a30a57",
   "metadata": {},
   "source": [
    "# 1. Descripción y Preparación del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7109f7",
   "metadata": {},
   "source": [
    "En esta sección se realizará la carga, exploración y preparación inicial del dataset seleccionado.  \n",
    "\n",
    "El objetivo es **asegurar la calidad y consistencia de los datos** antes de aplicar algoritmos de detección de comunidades.  \n",
    "\n",
    "Para ello se trabajará con el conjunto **“Wikipedia network of top categories”**, obtenido del repositorio [SNAP](https://snap.stanford.edu/data/), el cual representa la red de hipervínculos entre páginas de Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df97769",
   "metadata": {},
   "source": [
    "**1.1 Carga del Dataset**\n",
    "\n",
    " \n",
    "Primero se importan las librerías necesarias y se carga el dataset desde el archivo `wiki-topcats.txt`, el cual contiene las conexiones (aristas) entre páginas de Wikipedia (nodos).  \n",
    "Esta etapa permite verificar que los datos se leen correctamente y obtener una visión general del tamaño de la red.\n",
    "\n",
    "  \n",
    "La carga inicial y la revisión básica permiten detectar posibles errores de formato, valores faltantes o problemas de codificación que puedan afectar los análisis posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2662449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado correctamente\n",
      "Total de filas (enlaces): 28,511,807\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>170193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>598775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  target\n",
       "0       0   10772\n",
       "1       1       2\n",
       "2       1  170193\n",
       "3       1  598775\n",
       "4       2       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Análisis de la Red ===\n",
      "\n",
      "Número total de enlaces (conexiones entre páginas): 28,511,807\n",
      "Número total de nodos (páginas únicas de Wikipedia): 1,791,489\n",
      "Número total de enlaces (conexiones entre páginas): 28,511,807\n",
      "Número total de nodos (páginas únicas de Wikipedia): 1,791,489\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNúmero total de nodos (páginas únicas de Wikipedia): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_nodos\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 2. Análisis de conectividad y estructura\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mfrom_pandas_edgelist(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Métricas básicas\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Métricas de la Red ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:3\u001b[0m, in \u001b[0;36margmap_from_pandas_edgelist_1\u001b[1;34m(df, source, target, edge_attr, create_using, edge_key, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\a7xsl\\anaconda3\\Lib\\site-packages\\networkx\\utils\\backends.py:967\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[1;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetworkx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m backend is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001b[39;00m\n\u001b[0;32m    972\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[1;32mc:\\Users\\a7xsl\\anaconda3\\Lib\\site-packages\\networkx\\convert_matrix.py:439\u001b[0m, in \u001b[0;36mfrom_pandas_edgelist\u001b[1;34m(df, source, target, edge_attr, create_using, edge_key)\u001b[0m\n\u001b[0;32m    437\u001b[0m             g\u001b[38;5;241m.\u001b[39madd_edge(u, v, k)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m         g\u001b[38;5;241m.\u001b[39madd_edges_from(\u001b[38;5;28mzip\u001b[39m(df[source], df[target]))\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m g\n\u001b[0;32m    442\u001b[0m reserved_columns \u001b[38;5;241m=\u001b[39m [source, target]\n",
      "File \u001b[1;32mc:\\Users\\a7xsl\\anaconda3\\Lib\\site-packages\\networkx\\classes\\graph.py:1049\u001b[0m, in \u001b[0;36mGraph.add_edges_from\u001b[1;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjlist_inner_dict_factory()\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_attr_dict_factory()\n\u001b[1;32m-> 1049\u001b[0m datadict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u]\u001b[38;5;241m.\u001b[39mget(v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_attr_dict_factory())\n\u001b[0;32m   1050\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(attr)\n\u001b[0;32m   1051\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(dd)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Cargar el dataset\n",
    "    df = pd.read_csv('wiki-topcats.txt', sep=r'\\s+', header=None, names=['source', 'target'])\n",
    "\n",
    "    print(\"Dataset cargado correctamente\")\n",
    "    print(f\"Total de filas (enlaces): {len(df):,}\")\n",
    "    print(f\"Primeras filas del dataset:\")\n",
    "    display(df.head())\n",
    "\n",
    "    \n",
    "    # 1. Análisis básico de la red\n",
    "    print(\"=== Análisis de la Red ===\\n\")\n",
    "    num_enlaces = len(df)\n",
    "    num_nodos = len(set(df['source']).union(set(df['target'])))\n",
    "    print(f\"Número total de enlaces (conexiones entre páginas): {num_enlaces:,}\")\n",
    "    print(f\"Número total de nodos (páginas únicas de Wikipedia): {num_nodos:,}\")\n",
    "    \n",
    "    # 2. Análisis de conectividad y estructura\n",
    "    G = nx.from_pandas_edgelist(df, 'source', 'target')\n",
    "    \n",
    "    # Métricas básicas\n",
    "    print(\"\\n=== Métricas de la Red ===\")\n",
    "    print(f\"Tipo de red: {'Dirigida' if G.is_directed() else 'No dirigida'} (No dirigida significa que los enlaces son bidireccionales)\")\n",
    "    densidad = nx.density(G)\n",
    "    print(f\"Densidad: {densidad:.6f} (Porcentaje de conexiones existentes del total posible: {densidad*100:.4f}%)\")\n",
    "    \n",
    "    # 3. Análisis de grado\n",
    "    grados = dict(G.degree())\n",
    "    grado_promedio = sum(grados.values()) / len(grados)\n",
    "    grado_max = max(grados.values())\n",
    "    grado_min = min(grados.values())\n",
    "    \n",
    "    print(\"\\n=== Estadísticas de Grado (número de conexiones por página) ===\")\n",
    "    print(f\"Grado promedio: {grado_promedio:.2f} enlaces por página\")\n",
    "    print(f\"Grado máximo: {grado_max} (página más conectada)\")\n",
    "    print(f\"Grado mínimo: {grado_min} (página menos conectada)\")\n",
    "\n",
    "    # Análisis de componentes y estructura\n",
    "    print(\"\\n=== Análisis de Componentes (grupos conectados) ===\")\n",
    "    num_componentes = nx.number_connected_components(G)\n",
    "    componente_gigante = max(nx.connected_components(G), key=len)\n",
    "    tam_componente_gigante = len(componente_gigante)\n",
    "    porcentaje_gigante = (tam_componente_gigante/G.number_of_nodes())*100\n",
    "    \n",
    "    print(f\"Número de componentes conectados: {num_componentes}\")\n",
    "    print(f\"Tamaño de la componente gigante: {tam_componente_gigante:,} páginas\")\n",
    "    print(f\"Porcentaje de páginas en la componente gigante: {porcentaje_gigante:.2f}%\")\n",
    "    if porcentaje_gigante == 100:\n",
    "        print(\"Interpretación: Todas las páginas están conectadas entre sí, formando una única red navegable.\")\n",
    "    else:\n",
    "        print(f\"Interpretación: El {porcentaje_gigante:.2f}% de las páginas forman un grupo conectado principal.\")\n",
    "    \n",
    "    # 4. Visualización de la distribución de grado\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    \n",
    "    # Crear figura con dos subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histograma normal\n",
    "    ax1.hist(degrees, bins=50, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('Distribución de Grado (escala normal)')\n",
    "    ax1.set_xlabel('Grado (número de enlaces)')\n",
    "    ax1.set_ylabel('Frecuencia (número de páginas)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histograma log-log\n",
    "    # Calcular el histograma\n",
    "    hist, bins = np.histogram(degrees, bins=50)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    \n",
    "    # Filtrar valores mayores que 0 para la escala log\n",
    "    mask = hist > 0\n",
    "    ax2.scatter(centers[mask], hist[mask], color='blue', alpha=0.6, s=50)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_title('Distribución de Grado (escala log-log)')\n",
    "    ax2.set_xlabel('Grado (log)')\n",
    "    ax2.set_ylabel('Frecuencia (log)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontró el archivo wiki-topcats.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al analizar el dataset: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f3467",
   "metadata": {},
   "source": [
    "## 1.3 Preparación de Datos\n",
    "* Limpieza de datos\n",
    "  - Eliminación de duplicados\n",
    "  - Manejo de valores faltantes\n",
    "  - Corrección de formatos\n",
    "* Normalización\n",
    "  - Estandarización de identificadores\n",
    "  - Conversión de tipos de datos\n",
    "  - Unificación de formatos\n",
    "* Validación\n",
    "  - Verificación de integridad\n",
    "  - Control de consistencia\n",
    "  - Detección de anomalías\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Lectura y validación de datos\n",
    "* Limpieza y normalización\n",
    "* Creación de estructura de grafo\n",
    "* Verificación de integridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c07b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_datos(archivo):\n",
    "    try:\n",
    "        # 1. Carga inicial\n",
    "        print(\"=== Cargando y limpiando datos ===\")\n",
    "        df = pd.read_csv(archivo, sep=r'\\s+', header=None, names=['source', 'target'])\n",
    "        print(f\"Registros originales: {len(df)}\")\n",
    "        \n",
    "        # 2. Limpieza básica\n",
    "        # Eliminar duplicados\n",
    "        df_clean = df.drop_duplicates()\n",
    "        print(f\"Registros después de eliminar duplicados: {len(df_clean)}\")\n",
    "        \n",
    "        # Eliminar self-loops\n",
    "        df_clean = df_clean[df_clean['source'] != df_clean['target']]\n",
    "        print(f\"Registros después de eliminar self-loops: {len(df_clean)}\")\n",
    "        \n",
    "        # 3. Validación de datos\n",
    "        # Verificar valores nulos o vacíos\n",
    "        nulos = df_clean.isnull().sum()\n",
    "        if nulos.any():\n",
    "            print(\"\\nValores nulos encontrados:\")\n",
    "            print(nulos)\n",
    "            df_clean = df_clean.dropna()\n",
    "            print(f\"Registros después de eliminar nulos: {len(df_clean)}\")\n",
    "        \n",
    "        # 4. Crear grafo limpio\n",
    "        G_clean = nx.from_pandas_edgelist(df_clean, 'source', 'target')\n",
    "        \n",
    "        # 5. Estadísticas de limpieza\n",
    "        print(\"\\n=== Estadísticas después de limpieza ===\")\n",
    "        print(f\"Nodos: {G_clean.number_of_nodes():,}\")\n",
    "        print(f\"Enlaces: {G_clean.number_of_edges():,}\")\n",
    "        print(f\"Densidad: {nx.density(G_clean):.6f}\")\n",
    "        \n",
    "        return df_clean, G_clean\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en la preparación de datos: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Descomentar la siguiente línea para ejecutar la preparación de datos\n",
    "# df_limpio, G_limpio = preparar_datos('wiki-topcats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e78674",
   "metadata": {},
   "source": [
    "## 1.5 Muestreo Estratificado por Grado\n",
    "\n",
    "Para trabajar eficientemente con el dataset de 1.7 millones de nodos, realizamos un **muestreo estratificado por grado** que:\n",
    "- Selecciona el 5% de los nodos (aproximadamente 89,500)\n",
    "- Mantiene representatividad: ALTO, MEDIO y BAJO grado\n",
    "- Preserva las propiedades estadísticas principales\n",
    "- Prepara datos para análisis detallados\n",
    "\n",
    "**Estrategia:** Dividir los nodos en tres grupos según su grado (conexiones) y tomar una muestra equilibrada de cada grupo. Esto asegura que la muestra contenga nodos muy conectados, moderadamente conectados y poco conectados, reflejando la diversidad de la red original.\n",
    "\n",
    "**Técnicas implementadas:**\n",
    "- Muestreo estratificado por grado (selección de ALTO, MEDIO, BAJO grado)\n",
    "- Validación de representatividad (comparar densidades)\n",
    "- Control de calidad (tamaño de muestra y preservación de propiedades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8292e7",
   "metadata": {},
   "source": [
    "# 2. Análisis de Estructura de Red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251ad70",
   "metadata": {},
   "source": [
    "## 2.1 Métricas Básicas (Dataset Completo)\n",
    "\n",
    "En esta sección analizamos características rápidas del dataset completo:\n",
    "\n",
    "* Densidad de la red\n",
    "* Distribución de grado (histogramas)\n",
    "* Estadísticas de grado (mínimo, máximo, promedio)\n",
    "* Componentes conectados y componente gigante\n",
    "* Visualización de distribuciones\n",
    "\n",
    "**Ventaja:** Estos cálculos son rápidos incluso para 1.7 millones de nodos y dan una visión general de la estructura de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe3b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MUESTREO ESTRATIFICADO DEL DATASET\n",
      "======================================================================\n",
      "\n",
      "Dataset (G) no disponible. Ejecuta primero la celda de carga de datos.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MUESTREO ESTRATIFICADO DEL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'G' not in globals() or G is None or G.number_of_nodes() == 0:\n",
    "    print('\\nDataset (G) no disponible. Ejecuta primero la celda de carga de datos.')\n",
    "else:\n",
    "    try:\n",
    "        # Configuración del muestreo\n",
    "        tamano_total = G.number_of_nodes()\n",
    "        porcentaje_muestra = 0.05  # 5% de los nodos\n",
    "        cantidad_nodos_muestra = int(tamano_total * porcentaje_muestra)\n",
    "        \n",
    "        print(f'\\nConfiguracion de la Muestra:')\n",
    "        print(f'  Tamaño total del dataset: {tamano_total:,} nodos')\n",
    "        print(f'  Porcentaje muestreado: {porcentaje_muestra*100}%')\n",
    "        print(f'  Cantidad de nodos a muestrear: {cantidad_nodos_muestra:,} nodos')\n",
    "        \n",
    "        # Muestreo estratificado por grado\n",
    "        print(f'\\nMuestreo Estratificado por Grado:')\n",
    "        print('  Estrategia: Seleccionar nodos de ALTO, MEDIO y BAJO grado')\n",
    "        print('  Razon: Representar la diversidad de conectividad en la red\\n')\n",
    "        \n",
    "        grados = dict(G.degree())\n",
    "        nodos_ordenados = sorted(grados.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Dividir en 3 grupos: alto, medio, bajo\n",
    "        tercio = cantidad_nodos_muestra // 3\n",
    "        \n",
    "        alto_grado = [n for n, _ in nodos_ordenados[:tercio]]\n",
    "        medio_grado = [n for n, _ in nodos_ordenados[len(nodos_ordenados)//2:len(nodos_ordenados)//2 + tercio]]\n",
    "        bajo_grado = [n for n, _ in nodos_ordenados[-tercio:]]\n",
    "        \n",
    "        nodos_muestra = alto_grado + medio_grado + bajo_grado\n",
    "        G_muestra = G.subgraph(nodos_muestra).copy()\n",
    "        \n",
    "        print(f'Nodos de ALTO grado seleccionados: {len(alto_grado):,}')\n",
    "        print(f'  Grado minimo en este grupo: {min([grados[n] for n in alto_grado]):,}')\n",
    "        print(f'  Grado maximo en este grupo: {max([grados[n] for n in alto_grado]):,}')\n",
    "        \n",
    "        print(f'\\nNodos de GRADO MEDIO seleccionados: {len(medio_grado):,}')\n",
    "        print(f'  Grado minimo en este grupo: {min([grados[n] for n in medio_grado]):,}')\n",
    "        print(f'  Grado maximo en este grupo: {max([grados[n] for n in medio_grado]):,}')\n",
    "        \n",
    "        print(f'\\nNodos de BAJO grado seleccionados: {len(bajo_grado):,}')\n",
    "        print(f'  Grado minimo en este grupo: {min([grados[n] for n in bajo_grado]):,}')\n",
    "        print(f'  Grado maximo en este grupo: {max([grados[n] for n in bajo_grado]):,}')\n",
    "        \n",
    "        print(f'\\nResumen de la Muestra:')\n",
    "        print(f'  Total de nodos en muestra: {G_muestra.number_of_nodes():,}')\n",
    "        print(f'  Total de aristas en muestra: {G_muestra.number_of_edges():,}')\n",
    "        print(f'  Densidad de muestra: {nx.density(G_muestra):.6f}')\n",
    "        print(f'  Densidad original: {nx.density(G):.6f}')\n",
    "        \n",
    "        print(f'\\nG_muestra creada exitosamente y lista para analisis detallado')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'\\nError en muestreo: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c249f",
   "metadata": {},
   "source": [
    "## 2.2 Análisis Detallado (Muestra)\n",
    "\n",
    "En esta sección analizamos propiedades que requieren mayor poder computacional. Estos analisis se ejecutan en la muestra estratificada por ser muy intensivos en el dataset completo.\n",
    "\n",
    "Propiedades a analizar:\n",
    "* Coeficiente de clustering (agrupamiento local)\n",
    "* Triangulos y patrones de conectividad\n",
    "* Centralidades avanzadas:\n",
    "  - Centralidad de grado (nodos con mas conexiones)\n",
    "  - Centralidad de intermediacion (nodos que actuan como puentes)\n",
    "  - Centralidad de cercania (nodos cercanos al centro de la red)\n",
    "* Identificacion de nodos clave\n",
    "\n",
    "**Nota:** Los calculos se ejecutan automaticamente cuando G_muestra este disponible (despues del muestreo estratificado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbf68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecuta la celda de carga de datos primero.\n"
     ]
    }
   ],
   "source": [
    "if 'G' not in globals() or G is None or G.number_of_nodes() == 0:\n",
    "    print('Ejecuta la celda de carga de datos primero.')\n",
    "else:\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    print(f'\\n=== MÉTRICAS BÁSICAS (Dataset Completo) ===')\n",
    "    print(f'Nodos: {n:,}')\n",
    "    print(f'Aristas: {m:,}')\n",
    "    \n",
    "    # Grado\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    print(f'\\n=== Estadísticas de Grado ===')\n",
    "    print(f'Grado mínimo: {min(degrees):,}')\n",
    "    print(f'Grado máximo: {max(degrees):,}')\n",
    "    print(f'Grado promedio: {round(statistics.mean(degrees), 2)}')\n",
    "    \n",
    "    # Densidad\n",
    "    print(f'\\n=== Densidad ===')\n",
    "    print(f'Densidad: {nx.density(G):.6f}')\n",
    "\n",
    "    # Componentes\n",
    "    print(f'\\n=== Componentes Conectados ===')\n",
    "    comps = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    print(f'Número de componentes: {len(comps)}')\n",
    "    print(f'Tamaño de la componente gigante: {len(comps[0]):,}')\n",
    "    print(f'Porcentaje en componente gigante: {len(comps[0])/n*100:.2f}%')\n",
    "\n",
    "    # Visualización de distribución de grado\n",
    "    print(f'\\n=== Visualización: Distribución de Grado ===')\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histograma normal\n",
    "        ax1.hist(degrees, bins=50, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Distribución de Grado (escala normal)')\n",
    "        ax1.set_xlabel('Grado (número de enlaces)')\n",
    "        ax1.set_ylabel('Frecuencia')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Histograma log-log\n",
    "        hist, bins = np.histogram(degrees, bins=50)\n",
    "        centers = (bins[:-1] + bins[1:]) / 2\n",
    "        mask = hist > 0\n",
    "        ax2.scatter(centers[mask], hist[mask], color='blue', alpha=0.6, s=50)\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.set_title('Distribución de Grado (escala log-log)')\n",
    "        ax2.set_xlabel('Grado (log)')\n",
    "        ax2.set_ylabel('Frecuencia (log)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Error en visualización: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4620d97",
   "metadata": {},
   "source": [
    "# 3. Métodos de Detección de Comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36e4338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANÁLISIS DETALLADO (MUESTRA)\n",
      "======================================================================\n",
      "\n",
      "G_muestra no esta disponible. Ejecuta primero:\n",
      "  1. Carga de datos\n",
      "  2. Muestreo estratificado\n",
      "\n",
      "Este analisis se ejecutara automaticamente cuando G_muestra este lista.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISIS DETALLADO (MUESTRA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar si existe la muestra\n",
    "if 'G_muestra' not in globals() or G_muestra is None or G_muestra.number_of_nodes() == 0:\n",
    "    print('\\nG_muestra no esta disponible. Ejecuta primero:')\n",
    "    print('  1. Carga de datos')\n",
    "    print('  2. Muestreo estratificado')\n",
    "    print('\\nEste analisis se ejecutara automaticamente cuando G_muestra este lista.')\n",
    "else:\n",
    "    print('\\nMuestra detectada. Ejecutando analisis detallado...\\n')\n",
    "    \n",
    "    # Información de la muestra\n",
    "    n_muestra = G_muestra.number_of_nodes()\n",
    "    m_muestra = G_muestra.number_of_edges()\n",
    "    print('Información de la Muestra:')\n",
    "    print(f'  Nodos en muestra: {n_muestra:,}')\n",
    "    print(f'  Aristas en muestra: {m_muestra:,}')\n",
    "    \n",
    "    # Clustering\n",
    "    print('\\nAnalisis de Clustering:')\n",
    "    try:\n",
    "        coef_clustering = nx.average_clustering(G_muestra)\n",
    "        print(f'  Coeficiente de clustering promedio: {coef_clustering:.6f}')\n",
    "        print('  Interpretacion: Mide que tan agrupados estan los nodos')\n",
    "        print('    - 0 = sin agrupamiento local')\n",
    "        print('    - 1 = maximo agrupamiento (todos conectados)')\n",
    "    except Exception as e:\n",
    "        print(f'  Error calculando clustering: {e}')\n",
    "    \n",
    "    # Triangulos\n",
    "    print('\\nAnalisis de Triangulos:')\n",
    "    try:\n",
    "        triangles = nx.triangles(G_muestra)\n",
    "        num_triangles = sum(triangles.values()) // 3\n",
    "        print(f'  Numero total de triangulos: {num_triangles:,}')\n",
    "        print('  Interpretacion: Triangulos son 3 nodos mutuamente conectados')\n",
    "    except Exception as e:\n",
    "        print(f'  Error calculando triangulos: {e}')\n",
    "    \n",
    "    # Centralidades\n",
    "    print('\\nAnalisis de Centralidades:')\n",
    "    \n",
    "    # Centralidad de grado\n",
    "    print('\\n  Centralidad de Grado:')\n",
    "    try:\n",
    "        deg_cent = nx.degree_centrality(G_muestra)\n",
    "        top_deg = sorted(deg_cent.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print('    Top 10 nodos por centralidad de grado:')\n",
    "        for i, (node, val) in enumerate(top_deg, 1):\n",
    "            print(f'      {i:2d}. Nodo {node}: {val:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'    Error: {e}')\n",
    "    \n",
    "    # Centralidad de intermediación (betweenness)\n",
    "    print('\\n  Centralidad de Intermediacion (Betweenness):')\n",
    "    try:\n",
    "        print('    Calculando (puede tomar tiempo)...')\n",
    "        betweenness = nx.betweenness_centrality(G_muestra)\n",
    "        top_bet = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print('    Top 10 nodos (puentes importantes):')\n",
    "        for i, (node, val) in enumerate(top_bet, 1):\n",
    "            print(f'      {i:2d}. Nodo {node}: {val:.6f}')\n",
    "        print('    Interpretacion: Nodos que actuan como puentes entre comunidades')\n",
    "    except Exception as e:\n",
    "        print(f'    Error: {e}')\n",
    "    \n",
    "    # Centralidad de cercania (closeness)\n",
    "    print('\\n  Centralidad de Cercania (Closeness):')\n",
    "    try:\n",
    "        print('    Calculando...')\n",
    "        closeness = nx.closeness_centrality(G_muestra)\n",
    "        top_close = sorted(closeness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print('    Top 10 nodos mas cercanos al centro:')\n",
    "        for i, (node, val) in enumerate(top_close, 1):\n",
    "            print(f'      {i:2d}. Nodo {node}: {val:.6f}')\n",
    "        print('    Interpretacion: Nodos que alcanzan rapidamente a otros nodos')\n",
    "    except Exception as e:\n",
    "        print(f'    Error: {e}')\n",
    "    \n",
    "    # Validacion de muestra\n",
    "    print('\\nValidacion de Representatividad de Muestra:')\n",
    "    try:\n",
    "        densidad_muestra = nx.density(G_muestra)\n",
    "        densidad_original = nx.density(G)\n",
    "        print(f'  Densidad - Muestra: {densidad_muestra:.6f}')\n",
    "        print(f'  Densidad - Original: {densidad_original:.6f}')\n",
    "        diff_relativa = abs(densidad_muestra - densidad_original) / densidad_original * 100\n",
    "        print(f'  Diferencia relativa: {diff_relativa:.2f}%')\n",
    "        if diff_relativa < 10:\n",
    "            print('  Conclusion: La muestra representa bien el dataset original')\n",
    "        else:\n",
    "            print('  Conclusion: La muestra difiere moderadamente del dataset original')\n",
    "    except Exception as e:\n",
    "        print(f'  Error en validacion: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0db01a",
   "metadata": {},
   "source": [
    "## 3.2 Detección de Comunidades (Muestra)\n",
    "\n",
    "Los algoritmos de detección de comunidades se aplican a la muestra estratificada para identificar grupos de nodos altamente interconectados.\n",
    "\n",
    "**Algoritmos implementados:**\n",
    "* Louvain - Optimizacion de modularidad (rapido, recomendado)\n",
    "* Label Propagation - Propagacion de etiquetas (muy rapido, sin hiperparametros)\n",
    "* Girvan-Newman - Demo en subgrafo (para ilustracion de estructura jerarquica)\n",
    "\n",
    "**Criterios de selección:**\n",
    "- Louvain: Balance entre velocidad y calidad de resultados\n",
    "- Label Propagation: Cuando necesitas rapidez extrema\n",
    "- Girvan-Newman: Para entender la estructura jerarquica en grafos pequeños\n",
    "\n",
    "**Nota:** En esta sección se implementan los algoritmos de detección en la muestra, se comparan resultados y se evalua el desempeño computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d128f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DETECCIÓN DE COMUNIDADES\n",
      "======================================================================\n",
      "\n",
      "G_muestra no esta disponible. Ejecuta primero:\n",
      "  1. Carga de datos\n",
      "  2. Muestreo estratificado\n",
      "\n",
      "Este analisis se ejecutara automaticamente cuando G_muestra este lista.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECCIÓN DE COMUNIDADES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar si existe la muestra\n",
    "if 'G_muestra' not in globals() or G_muestra is None or G_muestra.number_of_nodes() == 0:\n",
    "    print('\\nG_muestra no esta disponible. Ejecuta primero:')\n",
    "    print('  1. Carga de datos')\n",
    "    print('  2. Muestreo estratificado')\n",
    "    print('\\nEste analisis se ejecutara automaticamente cuando G_muestra este lista.')\n",
    "else:\n",
    "    print('\\nMuestra detectada. Ejecutando deteccion de comunidades...\\n')\n",
    "    \n",
    "    partition = None\n",
    "    \n",
    "    # Algoritmo de Louvain\n",
    "    print('Algoritmo de Louvain:')\n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        partition = community_louvain.best_partition(G_muestra)\n",
    "        counts = Counter(partition.values())\n",
    "        print(f'  Comunidades detectadas: {len(counts)}')\n",
    "        print(f'  Top 5 comunidades por tamaño:')\n",
    "        for i, (comm_id, count) in enumerate(counts.most_common(5), 1):\n",
    "            print(f'    {i}. Comunidad {comm_id}: {count:,} nodos')\n",
    "        print('  Louvain completado exitosamente')\n",
    "    except ImportError:\n",
    "        print('  Advertencia: python-louvain no instalado.')\n",
    "        print('  Instalar con: pip install python-louvain')\n",
    "        partition = None\n",
    "    except Exception as e:\n",
    "        print(f'  Error en Louvain: {e}')\n",
    "        partition = None\n",
    "\n",
    "    # Algoritmo de Label Propagation\n",
    "    print('\\nAlgoritmo de Label Propagation:')\n",
    "    try:\n",
    "        from networkx.algorithms.community import label_propagation\n",
    "        lp_comms = list(label_propagation.label_propagation_communities(G_muestra))\n",
    "        sizes_lp = [len(c) for c in lp_comms]\n",
    "        print(f'  Comunidades detectadas: {len(lp_comms)}')\n",
    "        print(f'  Tamaños: min={min(sizes_lp):,}, max={max(sizes_lp):,}, promedio={sum(sizes_lp)/len(sizes_lp):.0f}')\n",
    "        print('  Label Propagation completado exitosamente')\n",
    "    except Exception as e:\n",
    "        print(f'  Error en Label Propagation: {e}')\n",
    "\n",
    "    # Algoritmo de Girvan-Newman (Demo en subgrafo)\n",
    "    print('\\nAlgoritmo de Girvan-Newman (Demo):')\n",
    "    try:\n",
    "        from networkx.algorithms import community as nx_comm\n",
    "        # Tomar los 500 nodos de mayor grado como demo\n",
    "        nodes_top = list(sorted(G_muestra.nodes(), key=lambda n: G_muestra.degree(n), reverse=True))[:500]\n",
    "        G_gn = G_muestra.subgraph(nodes_top).copy()\n",
    "        comp_gen = nx_comm.girvan_newman(G_gn)\n",
    "        top = tuple(sorted(c) for c in next(comp_gen))\n",
    "        print(f'  Demo en subgrafo de {len(nodes_top)} nodos')\n",
    "        print(f'  Comunidades en primer corte: {len(top)}')\n",
    "        sizes_gn = [len(c) for c in top]\n",
    "        print(f'  Tamaños: min={min(sizes_gn):,}, max={max(sizes_gn):,}')\n",
    "        print('  Girvan-Newman demo completado')\n",
    "    except Exception as e:\n",
    "        print(f'  Error en Girvan-Newman: {e}')\n",
    "    \n",
    "    # Guardar partition para uso posterior\n",
    "    if partition:\n",
    "        print('\\nParticion guardada como variable global \"partition\" para analisis posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b6b30",
   "metadata": {},
   "source": [
    "# 4. Visualización de Comunidades\n",
    "\n",
    "Técnicas efectivas para visualizar y comprender las estructuras comunitarias:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27bc82b",
   "metadata": {},
   "source": [
    "## 4.1 Técnicas de Visualización\n",
    "* Layouts de fuerza dirigida\n",
    "* Visualización jerárquica\n",
    "* Mapas de calor\n",
    "* Representaciones circulares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc2b63",
   "metadata": {},
   "source": [
    "## 4.2 Estrategias de Diseño\n",
    "* Codificación por colores\n",
    "* Agrupamiento visual\n",
    "* Escalado de nodos\n",
    "* Filtrado interactivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de81e3",
   "metadata": {},
   "source": [
    "## 4.3 Interpretación Visual\n",
    "* Patrones estructurales\n",
    "* Roles de nodos\n",
    "* Conexiones entre comunidades\n",
    "* Anomalías y casos especiales\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Creación de visualizaciones\n",
    "* Personalización de layouts\n",
    "* Interactividad\n",
    "* Exportación de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3bf20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VISUALIZACIÓN DE COMUNIDADES\n",
      "======================================================================\n",
      "\n",
      "G_muestra no disponible. Ejecuta primero:\n",
      "  1. Carga de datos\n",
      "  2. Muestreo estratificado\n",
      "  3. Deteccion de comunidades\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZACIÓN DE COMUNIDADES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'G_muestra' not in globals() or G_muestra is None or G_muestra.number_of_nodes() == 0:\n",
    "    print('\\nG_muestra no disponible. Ejecuta primero:')\n",
    "    print('  1. Carga de datos')\n",
    "    print('  2. Muestreo estratificado')\n",
    "    print('  3. Deteccion de comunidades')\n",
    "else:\n",
    "    print('\\nMuestra disponible. Visualizando comunidades...\\n')\n",
    "    \n",
    "    try:\n",
    "        # Verificar si existe partition del algoritmo de Louvain\n",
    "        if 'partition' not in globals() or partition is None:\n",
    "            print('Usando componentes conectadas como comunidades (fallback)')\n",
    "            comm_map = {n: i for i, comp in enumerate(nx.connected_components(G_muestra)) for n in comp}\n",
    "        else:\n",
    "            print('Usando particion del algoritmo de Louvain')\n",
    "            comm_map = partition\n",
    "        \n",
    "        # Usar muestra completa para visualizar\n",
    "        nodes_viz = list(G_muestra.nodes())\n",
    "        unique_comms = sorted({comm_map.get(n, 0) for n in nodes_viz})\n",
    "        \n",
    "        print(f'Numero de comunidades a visualizar: {len(unique_comms)}')\n",
    "        \n",
    "        # Crear mapa de colores\n",
    "        cmap = plt.cm.get_cmap('tab20', max(1, len(unique_comms)))\n",
    "        color_list = [cmap(unique_comms.index(comm_map.get(n, 0))) for n in nodes_viz]\n",
    "        \n",
    "        # Layout de fuerza dirigida\n",
    "        print('Calculando layout de fuerza dirigida (puede tomar tiempo)...')\n",
    "        pos = nx.spring_layout(G_muestra, seed=42, k=0.1, iterations=30, scale=1)\n",
    "        \n",
    "        # Visualización\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        nx.draw_networkx_nodes(G_muestra, pos, node_size=30, node_color=color_list, alpha=0.8)\n",
    "        nx.draw_networkx_edges(G_muestra, pos, alpha=0.1, width=0.3, edge_color='gray')\n",
    "        plt.title('Muestra de Red - Coloreada por Comunidades', fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Estadísticas de comunidades\n",
    "        print('\\nEstadísticas de Comunidades:')\n",
    "        size_by_comm = Counter(comm_map.get(n, 0) for n in nodes_viz)\n",
    "        print(f'Total de comunidades: {len(size_by_comm)}')\n",
    "        print(f'\\nTop 10 comunidades por tamaño:')\n",
    "        for i, (comm_id, count) in enumerate(size_by_comm.most_common(10), 1):\n",
    "            print(f'  {i:2d}. Comunidad {comm_id}: {count:,} nodos ({count/len(nodes_viz)*100:.1f}%)')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error en visualizacion: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e10f41",
   "metadata": {},
   "source": [
    "# 4. Visualización y Evaluación de Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51f63",
   "metadata": {},
   "source": [
    "## 4.1 Visualización de Comunidades\n",
    "\n",
    "Tecnicas para visualizar y comprender la estructura de las comunidades:\n",
    "\n",
    "* Layouts de fuerza dirigida (spring layout)\n",
    "* Codificacion por colores segun comunidad\n",
    "* Estadisticas de tamaño de comunidades\n",
    "* Interpretacion visual de la estructura\n",
    "\n",
    "**Procedimiento:**\n",
    "1. Ejecutar deteccion de comunidades\n",
    "2. Generar layout con fuerza dirigida\n",
    "3. Colorear nodos segun comunidad detectada\n",
    "4. Mostrar estadisticas de tamaño y distribucion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3352e",
   "metadata": {},
   "source": [
    "## 4.2 Evaluación de Calidad de Comunidades\n",
    "\n",
    "Metricas internas y externas para evaluar la calidad de las comunidades detectadas:\n",
    "\n",
    "**Metricas Internas:**\n",
    "* Modularidad - Mide la calidad de la particion en comunidades\n",
    "* Conductancia - Mide que tan aislada esta cada comunidad\n",
    "* Densidad intra-comunidad - Densidad de enlaces dentro de comunidades\n",
    "\n",
    "**Metricas Externas (requiere ground truth):**\n",
    "* Indice de Rand Ajustado (ARI) - Compara particiones\n",
    "* Información Mutua Normalizada (NMI) - Mide similitud entre particiones\n",
    "* F-measure - Balance entre precision y recall\n",
    "\n",
    "**Nota:** Las metricas se calculan automaticamente cuando hay datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1c6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUACIÓN DE CALIDAD DE COMUNIDADES\n",
      "======================================================================\n",
      "\n",
      "G_muestra no disponible.\n",
      "Ejecuta primero: 1) Carga, 2) Muestreo, 3) Deteccion de comunidades\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUACIÓN DE CALIDAD DE COMUNIDADES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def conductance(G, community_nodes):\n",
    "    \"\"\"Calcula la conductancia de una comunidad (que tan bien esta separada)\"\"\"\n",
    "    cut = 0\n",
    "    volS = 0\n",
    "    for u in community_nodes:\n",
    "        volS += G.degree(u)\n",
    "        for v in G.neighbors(u):\n",
    "            if v not in community_nodes:\n",
    "                cut += 1\n",
    "    vol_rest = sum(dict(G.degree()).values()) - volS\n",
    "    denom = min(volS, vol_rest) if min(volS, vol_rest) > 0 else 1\n",
    "    return cut / denom\n",
    "\n",
    "if 'G_muestra' not in globals() or G_muestra is None or G_muestra.number_of_nodes() == 0:\n",
    "    print('\\nG_muestra no disponible.')\n",
    "    print('Ejecuta primero: 1) Carga, 2) Muestreo, 3) Deteccion de comunidades')\n",
    "elif 'partition' not in globals() or partition is None:\n",
    "    print('\\nParticion de comunidades no disponible.')\n",
    "    print('Ejecuta primero la celda de deteccion de comunidades (Louvain)')\n",
    "else:\n",
    "    print('\\nEvaluando calidad de comunidades detectadas...\\n')\n",
    "    \n",
    "    try:\n",
    "        # Modularidad\n",
    "        print('Modularidad (Louvain):')\n",
    "        try:\n",
    "            import community as community_louvain\n",
    "            mod = community_louvain.modularity(partition, G_muestra)\n",
    "            print(f'  Valor: {mod:.6f}')\n",
    "            print('  Interpretacion:')\n",
    "            if mod > 0.4:\n",
    "                print('    Excelente - Comunidades muy bien definidas')\n",
    "            elif mod > 0.3:\n",
    "                print('    Buena - Comunidades bien separadas')\n",
    "            elif mod > 0.1:\n",
    "                print('    Moderada - Algunas comunidades identificadas')\n",
    "            else:\n",
    "                print('    Baja - Comunidades poco definidas')\n",
    "        except Exception as e:\n",
    "            print(f'  Error: {e}')\n",
    "        \n",
    "        # Conductancia\n",
    "        print('\\nConductancia de Comunidades:')\n",
    "        print('  Metrica que mide que tan aislada esta cada comunidad')\n",
    "        print('  (Valores mas bajos = mejor separacion)\\n')\n",
    "        try:\n",
    "            comms = defaultdict(list)\n",
    "            for n, c in partition.items():\n",
    "                comms[c].append(n)\n",
    "            \n",
    "            conductances = []\n",
    "            top = sorted(comms.items(), key=lambda x: len(x[1]), reverse=True)[:5]\n",
    "            \n",
    "            print('  Top 5 comunidades mas grandes:')\n",
    "            for i, (cid, nodes) in enumerate(top, 1):\n",
    "                cond = conductance(G_muestra, set(nodes))\n",
    "                conductances.append(cond)\n",
    "                print(f'    {i}. Comunidad {cid}: {len(nodes):5} nodos, Conductancia: {cond:.6f}')\n",
    "            \n",
    "            print(f'\\n  Conductancia promedio (top 5): {sum(conductances)/len(conductances):.6f}')\n",
    "        except Exception as e:\n",
    "            print(f'  Error: {e}')\n",
    "        \n",
    "        # Metricas externas (si hay ground truth)\n",
    "        print('\\nMetricas Externas (Comparacion con Ground Truth):')\n",
    "        try:\n",
    "            from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "            if 'true_labels' in globals() and true_labels:\n",
    "                nodes_common = [n for n in G_muestra.nodes() if n in true_labels and n in partition]\n",
    "                if nodes_common:\n",
    "                    y_true = [true_labels[n] for n in nodes_common]\n",
    "                    y_pred = [partition[n] for n in nodes_common]\n",
    "                    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "                    ari = adjusted_rand_score(y_true, y_pred)\n",
    "                    print(f'  NMI (Información Mutua Normalizada): {nmi:.6f}')\n",
    "                    print(f'  ARI (Índice de Rand Ajustado): {ari:.6f}')\n",
    "                    print('  (Valores cercanos a 1 = mejor concordancia con ground truth)')\n",
    "                else:\n",
    "                    print('  No hay nodos comunes en true_labels y partition')\n",
    "            else:\n",
    "                print('  Ground truth (true_labels) no disponible')\n",
    "        except ImportError:\n",
    "            print('  sklearn no instalado. Para metricas externas: pip install scikit-learn')\n",
    "        except Exception as e:\n",
    "            print(f'  Error en metricas externas: {e}')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error general en evaluacion: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6836a",
   "metadata": {},
   "source": [
    "## 6.1 Dinámica de Comunidades\n",
    "* Nacimiento y muerte\n",
    "* Fusión y división\n",
    "* Crecimiento y contracción\n",
    "* Estabilidad y cambio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0573056",
   "metadata": {},
   "source": [
    "## 6.2 Métricas Temporales\n",
    "* Supervivencia de comunidades\n",
    "* Tasa de cambio\n",
    "* Persistencia de membresía\n",
    "* Evolución estructural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d79994",
   "metadata": {},
   "source": [
    "## 6.3 Patrones de Evolución\n",
    "* Ciclos de vida\n",
    "* Puntos de transición\n",
    "* Factores de cambio\n",
    "* Predicción de evolución\n",
    "\n",
    "**Nota**: En esta sección implementaremos:\n",
    "* Análisis de series temporales\n",
    "* Tracking de comunidades\n",
    "* Visualización de evolución\n",
    "* Predicción de cambios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a8840",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusión\n",
    "Este notebook servirá como guía para el análisis de comunidades en redes, con implementaciones prácticas, ejemplos y visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9996153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si tu edgelist incluye timestamps, usa slice_windows para crear snapshots por ventana.\n"
     ]
    }
   ],
   "source": [
    "def slice_windows(path, window_size=86400, max_lines=None):\n",
    "    windows = defaultdict(list)\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines and i>=max_lines: break\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                if len(parts) >= 2:\n",
    "                    windows[0].append((parts[0], parts[1]))\n",
    "                continue\n",
    "            try:\n",
    "                t = int(parts[2])\n",
    "            except:\n",
    "                continue\n",
    "            win = (t//window_size)*window_size\n",
    "            windows[win].append((parts[0], parts[1]))\n",
    "    return windows\n",
    "\n",
    "print('Si tu edgelist incluye timestamps, usa slice_windows para crear snapshots por ventana.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
